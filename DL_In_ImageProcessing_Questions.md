# Questions to Deep Learning for Image Processing And Analysis

<font size=4.5pt  face = 'georgia' style='Line-height :3'><div style='text-align: justify; '><ol>
<li> Tasks of Image processing and analysis (including computer vision) with examples.
<li> Advantages and Disadvantages of Using Deep Learning in Image Processing (Computer Vision).
<li> Advantages and Disadvantages of Alternative approach to Using Deep Learning in Image Processing (Computer Vision).  
<li> Advantages of using Convolution Neural Network (CNN) in Image Processing and Analysis (Computer Vision).      
<li> Structure of Convolution Neural Network (meaning of feature extractor (backbone) and types of head layers).
<li> Meaning of an Receptive Field in CNN
<li> Explaining how 2d-convolution layer work (Multiple Channel Multiple Kernel)
<li> Meaning of extending the 2d-convolution operation (padding, stride, dialed rate)    
<li> Types of Convolution and their aims:<ul>
    <li> Conventional (square) Convolution, 
    <li> Cascade Convolution, 
    <li> Grouped Convolution, 
    <li> Spatially Separable Convolution, 
    <li> Pointwise Convolution, 
    <li> Deepwise-Convolution (and Deepwise-Separable Convolution),
    <li> Pixel-shuffle (High Resolution) Convolution, 
    <li> Transposed Convolution. </ul>
<li> Meaning of the Local Pooling Operation in CNN (for instance max-pooling).
<li> Types of Local Pooling operations and their aims:<ul>
    <li> max-pooling,
    <li> average-pooling,  </ul> 
<li> Advantages of the Global Average Pooling in CNN in comparison with flatten operation. 
<li> Types of Upsampling Layers 
<li> Meaning of the Activation Function in Neural Networks
<li> Why we use Sigmoid and SoftMax in Last Layers for Classification tasks (Advantages and Disadvantages of Sigmoid Activation function).
<li> Advantages of Reflected Linear Unit (ReLU) in hidden layers of CNN.
<li> Types of ReLU: ReLU6, Leaky ReLU, Parametric ReLU, ELU, SELU, GELU, Swish, Mish and how do you think why they all need (Disadvantages of Conventional ReLU).
<li> Meaning of Weights initialization.
<li> Difference between Binary Classification, Multi-Class Classification and Multi-Label Classification, 
<li> Types of Loss Function for Classification:Binary Cross-Entropy, One-Hot Categorical Cross-Entropy, Sparse Categorical Cross-Entropy, Imbalance cases.
<li> Reasons of using Cross-Entropy with Logits (Binary Cross-Entropy with Logits and Categorical Cross-Entropy with Logits).
<li> Types of Regression Loss: L2, L1, L1smooth (Huber Loss)   
<li> Specificity of Loss function in Semantic Segmentation.
<li> Method of Neural Network Regularization: L2, data augmentation, 2d Dropout (spatial Dropout), Batch Normalization.
<li> Advantages and Disadvantages of Batch Normalization and why some times we need to use Layer Norm or Group Norm.
<li> Meaning of Cross Validation.
<li> Meaning of Learning rate choosing and scheduling.
<li> Types of Stochastic Gradient Descent (SGD): SGD with Momentum (Nesterov), RMS prop, ADAM - why they need to use. 
<li> The main tendencies in the modern state of CNN Architectures.
<li> The main idea of Network In Net (and inception layer).
<li> The main idea residual connection (skip connection and denseNet block).
<li> The main idea of MobileNet V2 block.
<li> The main idea of Squeeze and Excitation (and MobileNet V3 block).
<li> The main ideas under EfficientNet V1/V2. 
<li> The main idea of the Noisy Student training scheme and its variants.
<li> Advantages and Disadvantages of Visual Transformer Networks. 


```python

```
